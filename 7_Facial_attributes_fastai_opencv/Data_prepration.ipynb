{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code contains pre-processing steps done on [CelebA dataset](https://www.kaggle.com/jessicali9530/celeba-dataset) from Kaggle to prepare for multi-class facial attribute classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Boiler Plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T21:16:15.337275Z",
     "start_time": "2019-01-24T21:16:12.646114Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T21:16:18.042812Z",
     "start_time": "2019-01-24T21:16:15.342223Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6aa9db9d65408cb9dd5d814062c1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imutils\n",
    "import glob\n",
    "import cv2\n",
    "import shutil\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', 500)\n",
    "tqdm_notebook().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Downloading databse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We are going to download [CelebFaces Attributes(CelebA) dataset](https://www.kaggle.com/jessicali9530/celeba-dataset) from Kaggle. This dataset is excellent for training and testing models for face detection, particularly for recognizing facial attributes such as finding people with brown hair, are smiling, or wearing glasses. Images cover large pose variations, background clutter, diverse people, supported by a large number of images and rich annotations. This data was originally collected by researchers at MMLAB, The Chinese University of Hong Kong (specific reference in Acknowledgment section).\n",
    "\n",
    "**Content**\n",
    "\n",
    "- 202,599 number of face images of various celebrities\n",
    "- 10,177 unique identities, but names of identities are not given\n",
    "- 40 binary attribute annotations per image\n",
    "- 5 landmark locations\n",
    "\n",
    "We are going to use kaggle-cli to download data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T05:04:05.729302Z",
     "start_time": "2019-01-24T05:04:05.700665Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## !kaggle datasets download -d jessicali9530/celeba-datasetb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting frontal face from images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CelebA dataset contains images of faces which are taken from the side and with different orientation and zoom angle. The first step of preprocessing is you select images which in which front face is visible and isolate that part of the image alone for our model training of facial attributes. We are going to use OpenCV Haar Cascades to find the location of the face in the image and crop to only keep the facial parts of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T21:16:18.291354Z",
     "start_time": "2019-01-24T21:16:18.072519Z"
    }
   },
   "outputs": [],
   "source": [
    "## Loading Haar Cascade\n",
    "## Taken from https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T21:16:19.110303Z",
     "start_time": "2019-01-24T21:16:19.067567Z"
    }
   },
   "outputs": [],
   "source": [
    "def face_extractor(origin, destination, fc):\n",
    "    ## Importing image using open cv\n",
    "    img = cv2.imread(origin,1)\n",
    "\n",
    "    ## Resizing to constant width\n",
    "    img = imutils.resize(img, width=200)\n",
    "    \n",
    "    ## Finding actual size of image\n",
    "    H,W,_ = img.shape\n",
    "    \n",
    "    ## Converting BGR to RGB\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    ## Detecting faces on the image\n",
    "    face_coord = fc.detectMultiScale(gray,1.2,10,minSize=(50,50))\n",
    "    \n",
    "    ## If only one face is foung\n",
    "    if len(face_coord) == 1:\n",
    "        X, Y, w, h = face_coord[0]\n",
    "    \n",
    "    ## If no face found --> SKIP\n",
    "    elif len(face_coord)==0:\n",
    "        return None\n",
    "    \n",
    "    ## If multiple faces are found take the one with largest area\n",
    "    else:\n",
    "        max_val = 0\n",
    "        max_idx = 0\n",
    "        for idx in range(len(face_coord)):\n",
    "            _, _, w_i, h_i = face_coord[idx]\n",
    "            if w_i*h_i > max_val:\n",
    "                max_idx = idx\n",
    "                max_val = w_i*h_i\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            X, Y, w, h = face_coord[max_idx]\n",
    "    \n",
    "    ## Crop and export the image\n",
    "    img_cp = img[\n",
    "            max(0,Y - int(0.35*h)): min(Y + int(1.35*h), H),\n",
    "            max(0,X - int(w*0.35)): min(X + int(1.35*w), W)\n",
    "        ].copy()\n",
    "    \n",
    "    cv2.imwrite(destination, img_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T21:16:26.977196Z",
     "start_time": "2019-01-24T21:16:23.864329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202599\n"
     ]
    }
   ],
   "source": [
    "## Defining destination path\n",
    "path = '../data/celeba/faces/'\n",
    "\n",
    "## Finding all the images in the folder\n",
    "item_list = glob.glob('../data/celeba/imgs/*.jpg')\n",
    "print(len(item_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T05:10:20.445764Z",
     "start_time": "2019-01-24T05:09:00.264576Z"
    }
   },
   "outputs": [],
   "source": [
    "## Will run for about an hour and a half \n",
    "for org in tqdm_notebook(item_list):\n",
    "    face_extractor(origin = org, destination = path+org.split('/')[-1], fc=face_cascade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Findign all the images and separating in training and validation\n",
    "item_list = glob.glob(path+'*.jpg')\n",
    "\n",
    "for idx in tqdm_notebook(range(1,202600)):\n",
    "    if idx <= 182637:\n",
    "        destination = path+'training/'\n",
    "    else:\n",
    "        destination = path+'validation/'\n",
    "    try:\n",
    "        shutil.move(\n",
    "            path+str(idx).zfill(6)+'.jpg', \n",
    "            destination+str(idx).zfill(6)+'.jpg'\n",
    "        )\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining all label attributes\n",
    "label_df = pd.read_csv('../data/celeba/list_attr_celeba.csv')\n",
    "column_list = pd.Series(list(label_df.columns)[1:])\n",
    "\n",
    "def label_generator(row):\n",
    "    return(' '.join(column_list[[True if i==1 else False for i in row[column_list]]]))\n",
    "\n",
    "label_df['label'] = label_df.progress_apply(lambda x: label_generator(x), axis=1)\n",
    "label_df = label_df.loc[:,['image_id','label']]\n",
    "label_df.to_csv('../data/celeba/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attachhing label to correct file names\n",
    "item_list = glob.glob('../data/celeba/faces/*/*.jpg')\n",
    "item_df = pd.DataFrame({'image_name':pd.Series(item_list).apply(lambda x: '/'.join(x.split('/')[-2]))})\n",
    "item_df['image_id'] = item_df.image_name.apply(lambda x: x.split('/')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating final label set\n",
    "label_df = pd.read_csv('../data/celeba/labels.csv')\n",
    "label_df = label_df.merge(item_df, on='image_id', how='inner')\n",
    "label_df.rename(columns={'label':'tags'}, inplace=True)\n",
    "label_df.loc[:,['image_name','tags']].to_csv('../data/celeba/faces/labels.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
